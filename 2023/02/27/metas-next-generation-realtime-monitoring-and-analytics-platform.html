<!DOCTYPE html>
<html>
  <head>
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-LKBDWTJ60B"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-LKBDWTJ60B");
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Meta’s Next-generation Realtime Monitoring and Analytics Platform</title>
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />

    <meta http-equiv="pragma" content="no-cache" />
    <meta name="robots" content="all" />
    <meta name="MSSmartTagsPreventParsing" content="true" />
    <meta http-equiv="imagetoolbar" content="false" />

    <link href="/css/tufte.css" rel="stylesheet" />
    <link
      rel="alternate"
      type="application/atom+xml"
      title="Atom Feed for www.micahlerner.com"
      href="/atom.xml"
    />
    <link
      rel="alternate"
      type="application/atom+xml"
      title="RSS Feed for www.micahlerner.com"
      href="/feed.xml"
    />
    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="/assets/images/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="/assets/images/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="/assets/images/favicon-16x16.png"
    />
    <link rel="manifest" href="/assets/imagessite.webmanifest" />
  </head>

  <body>
    <article>
  <section>
    <header>
      <a href="/">
        <h3>micahlerner.com</h3>
      </a>
    </header>
  </section>
  <h1>Meta’s Next-generation Realtime Monitoring and Analytics Platform</h1>
  
  <h4>Published February 27, 2023</h4>
  <h5>
    Found something wrong?
    <a
      href="https://github.com/mlerner/mlerner.github.io/edit/master/_posts/2023-02-27-metas-next-generation-realtime-monitoring-and-analytics-platform.md"
      >Submit a pull request!</a
    >
  </h5>
  <section id="post-content">
       <p><a href="https://www.vldb.org/pvldb/vol15/p3522-mo.pdf">Meta’s Next-generation Realtime Monitoring and Analytics Platform</a></p>

<h2 id="what-is-the-research">What is the research?</h2>

<p>Many companies perform real time analytics in order to customize user experiences or alert on anomalous behavior - other examples from industry include Netflix’s <a href="https://netflixtechblog.com/open-sourcing-mantis-a-platform-for-building-cost-effective-realtime-operations-focused-5b8ff387813a">open source Mantis platform</a> or open source projects like <a href="https://beam.apache.org/">Apache Beam</a><label for="druid" class="margin-toggle sidenote-number"></label><input type="checkbox" id="druid" class="margin-toggle" /><span class="sidenote">Druid, the subject of one such system is the subject of a <a href="https://www.micahlerner.com/2022/05/15/druid-a-real-time-analytical-data-store.html">previous paper review</a>! </span>. Meta performed this function using a system called <em>Scuba</em>, described in published research from <a href="https://research.facebook.com/file/2964294030497318/scuba-diving-into-data-at-facebook.pdf">VLDB 2013</a>. While the existing system scaled to meet initial demand, over time operational and usability issues led to an internal push to evolve the system.</p>

<p>The newly published research describes the system that came from this work, codenamed <em>Kraken</em>. Unlike Scuba, Kraken optimizes for configurability, allowing users to specify which tradeoffs they want to make around their dataset - for example, faster availability of a dataset can be balanced against the consistency of query results<label for="concrete" class="margin-toggle sidenote-number"></label><input type="checkbox" id="concrete" class="margin-toggle" /><span class="sidenote">To make this more concrete, user queries in Scuba would go to one of several copies of dataset. Depending on how each deployment of Scuba happened to have ingested the data, repeating the same query, but to different deployments, could return “inconsitent results.” </span>.</p>

<p>The paper provides technical references to the underlying primitives that Kraken is built on - one of which, <a href="https://www.micahlerner.com/2022/01/08/shard-manager-a-generic-shard-management-framework-for-geo-distributed-applications.html">Shard Manager</a>, was a previous paper review. Beyond describing the architecture of the system, the paper also discusses the experience of replacing the existing system.</p>

<h2 id="background-and-motivation">Background and Motivation</h2>

<p>Before diving into the details of the implementation, the paper does a thorough explanation of the design space of systems like Scuba, focusing on six main facets:</p>

<ul>
  <li><em>Query performance</em>: how quickly does the system respond to users?</li>
  <li><em>Freshness</em>: how up to date is the data served by the system?</li>
  <li><em>Availability</em>: what does the system do under different failure modes? Is data available at all, or can the system return partial results in a degraded state?</li>
  <li><em>Dataset size</em>: how much data can the system actually store and serve in a performant way?</li>
  <li><em>Resource efficiency</em>: how much resources (and of what type) does a system require in order to operate? Are the resources used all of the time, or just when responding to queries?</li>
  <li><em>Features</em>: is the system configurable for different use cases? Does it offer features like backfills and a SQL-like interface?</li>
</ul>

<p>Different databases weight these facets according to the system’s intended use. The paper focuses on three main categories of tradeoffs:</p>

<ul>
  <li><em>Data-warehouses</em>, which store large amounts of data, and are not on the path of a user’s interactive session<label for="snowflake" class="margin-toggle sidenote-number"></label><input type="checkbox" id="snowflake" class="margin-toggle" /><span class="sidenote">Snowflake, <a href="https://www.micahlerner.com/2023/01/19/elastic-cloud-services-scaling-snowflakes-control-plane.html">described in a previous paper review</a>, is one such example. </span>.</li>
  <li><em>Online analytical processing (OLAP)</em> databases, used for analytics and medium-size datasets<label for="napa" class="margin-toggle sidenote-number"></label><input type="checkbox" id="napa" class="margin-toggle" /><span class="sidenote">The paper notes that many modern OLAP databases are configurable for specifics use cases - in particular <a href="https://research.google/pubs/pub50617/">Napa</a>. </span></li>
  <li><em>Real-time monitoring systems</em>, which optimize for quickly surfacing up-to-date datasets in as complete a form as possible.</li>
</ul>

<figure><img class="maincolumn-img" src="/assets/kraken/figure1.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<p>While each category makes fundamentally different tradeoffs, they all have similar high-level components in their read and write paths. For example, databases need to ingest data, and some may implement “write-side optimizations” (like creating different views of the underlying dataset<label for="materialized" class="margin-toggle sidenote-number"></label><input type="checkbox" id="materialized" class="margin-toggle" /><span class="sidenote">These are often called <a href="https://en.wikipedia.org/wiki/Materialized_view">materalized views</a>. <a href="https://materialize.com/guides/materialized-views/">This article</a> by Materialize also provides helpful context. </span>). Storage for the underlying dataset is important, although different applications have different availability requirements.</p>

<figure><img class="maincolumn-img" src="/assets/kraken/figure2.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<p>This context is helpful for understanding the <em>why</em> behind Scuba’s design, as well as  its potential shortcomings as a real-time monitoring system. For example, the initial system sacrified features like joins in order to support better query performance. It also optimized for the common case and built a single index, improving resource efficiency - at the same time, applications not using that index experienced slower query performance. Lastly, Scuba optimized for durability and availability by designing its engine to return results, even if those results were imperfect<label for="tail" class="margin-toggle sidenote-number"></label><input type="checkbox" id="tail" class="margin-toggle" /><span class="sidenote">This problem could happen due to machines randomly failing or performing poorly, similar to what is described in <a href="https://www.barroso.org/publications/TheTailAtScale.pdf">The Tail at Scale</a>. </span>.</p>

<p>In contrast, Kraken was designed to make different tradeoffs than Scuba from the outset, specifically focusing on improving user experience and reducing the need for maintenance.</p>

<p>For example, Kraken aimed to provide consistent results to user queries by limiting divergence between copies of a dataset. This approach was unlike Scuba’s, which stored multiple independently-updated copies, each subject to drift from factors like underlying machines going offline - consequently, user queries received significantly varying results depending on the copy they communicated with (even when running the same query).</p>

<p>On the operational complexity front, Kraken aimed to take advantage of automation that would limit complicated manual interventions for operations and scaling. For example, Scuba often required configuration changes to perform updates on underlying hardware. Similarly, insufficient load balancing and load spikes to machines in the system would regularly cause crashes, but scaling resources in response was non-trivial.</p>

<h2 id="what-are-the-papers-contributions">What are the paper’s contributions?</h2>

<p>The paper makes three main contributions:</p>

<ul>
  <li>Description of the previous system, characterizing the problem space and motivating improvements.</li>
  <li>Design and implementation of the new system, called <em>Kraken</em>, at scale.</li>
  <li>Evaluation of the system and a description of the migration to it over time.</li>
</ul>

<h2 id="how-does-the-system-work">How does the system work?</h2>

<p>Kraken can be broken down into three main components:</p>

<ul>
  <li><em>Real-time Ingestion</em>, which covers how the logs make it into the system and become available to query.</li>
  <li><em>Backup</em>, which allows persistence of the dataset for long periods of time in an optimized format.</li>
  <li><em>Reads</em>, by which user queries access the stored information.</li>
</ul>

<figure><img class="maincolumn-img" src="/assets/kraken/figure3.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<h3 id="ingestion">Ingestion</h3>

<p>To ingest data, Kraken implements three main components: <em>writing raw logs</em>, <em>reading and operating on the raw logs</em>, and <em>deployment of log data to machines</em>.</p>

<p><em>Scribe</em> is the first step in ingesting raw logs<label for="scribe" class="margin-toggle sidenote-number"></label><input type="checkbox" id="scribe" class="margin-toggle" /><span class="sidenote">There are a few great resources around Scribe, my favorite of which is <a href="https://www.usenix.org/conference/lisa18/presentation/braunschweig">The History of Logging @ Facebook</a>. There is also a post on Scribe from Meta’s engineering blog <a href="https://engineering.fb.com/2019/10/07/data-infrastructure/scribe/">here</a>. Scribe appears to at one point have been <a href="https://github.com/facebookarchive/scribe">open source</a>, but is now archived - my assumption is Meta still maintains an internal fork. </span>. Applications log individual entries to a partitioned dataset (called a category), and Scribe notifies downstream systems who are “tailing” the logs.</p>

<p>To reliably read and operate on incoming raw logs at Facebook scale, Kraken uses a distributed framework called <em>Turbine</em><label for="turbine" class="margin-toggle sidenote-number"></label><input type="checkbox" id="turbine" class="margin-toggle" /><span class="sidenote">The original Turbine paper is <a href="https://ieeexplore.ieee.org/document/9101738">here</a>. </span>. Turbine schedules “tailer” jobs over many different machines in Meta’s fleet, scaling by adjusting the number of tailers according to load, and rescheduling failed tailers on new machines. The primary job of each tailer job is transforming the logs into a structured and organized format named <em>RowBlocks</em> - the incoming data is not guaranteed to be the same structure as the output dataset surfaced to the user.</p>

<p>Before completing its processing of a <em>RowBlock</em>, a tailer needs to determine which machines should store the final product. This decision is based on two pieces of information about the <em>RowBlock</em> - the input dataset it is associated with, and the <em>partition</em> of the dataset.</p>

<p>The output of this calculation is a <em>ShardId</em> corresponding to the unit of scaling Kraken uses to respond to more requests (sharding is discussed in more detail further down). Kraken then uses the <em>ShardId</em> to write multiple copies of the <em>RowBlock</em> to an ordered, immutable, distributed log system called <a href="https://engineering.fb.com/2017/08/31/core-data/logdevice-a-distributed-data-store-for-logs/">LogDevice</a><label for="logdevice" class="margin-toggle sidenote-number"></label><input type="checkbox" id="logdevice" class="margin-toggle" /><span class="sidenote">LogDevice also <a href="https://logdevice.io/">appears to have been an open source project</a> at one point. </span> on machines spread across Meta’s network.</p>

<p>To make the distributed log available for user queries, machines in the fleet (called <em>leafs</em>) fetch it. Each <em>leaf</em> is assigned shards (a scaling unit representing a specific subcomponent of data) by <em>Shard Manager</em><label for="shardman" class="margin-toggle sidenote-number"></label><input type="checkbox" id="shardman" class="margin-toggle" /><span class="sidenote">My previous paper review on Shard Manager is <a href="https://www.micahlerner.com/2022/01/08/shard-manager-a-generic-shard-management-framework-for-geo-distributed-applications.html">here</a>. </span>. To assign shards to leafs, <em>Shard Manager</em> takes into account factors like the load on different parts of a dataset. If a shard is under heavy load, <em>Shard Manager</em> instructs more leaf nodes to maintain a copy of it. <em>Shard Manager</em> also handles data-management tasks, like removing shards of datasets that should no longer be stored (according to a configured retention policy).</p>

<h3 id="remote-backup-and-compaction">Remote Backup and Compaction</h3>

<p>New data entering the system is only temporarily stored in the distributed log - the <em>LogDevice</em> is a fixed size, so accepting new entries can only happen if data “ages out” of short-term storage. This eviction implementation poses a problem if users want to query the underlying entries - to solve this, Kraken backs up data using a <em>Backup Compaction Service (BCS)</em>.</p>

<p><em>BCS</em> periodically reads data from the <em>LogDevice</em>, combining multiple blocks, compressing them, and writing the output to a blob storage system. After completing the transfer, <em>BCS</em> creates an entry in the <em>LogDevice</em>. When performing further reads, leafs interpret this entry as an instruction to read previous data from blob storage, rather than from the distributed log.</p>

<figure><img class="maincolumn-img" src="/assets/kraken/figure4.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<p>The backup and compaction process increases storage effiency (due to the compression technique), while lowering disk IO (to read the same amount of data, Kraken leaf nodes can perorm fewer disk accesses due to the larger individual size).</p>

<h3 id="read-path">Read Path</h3>

<p>When a user issues a query for data stored in Kraken, a <em>root node</em> executes the query by parallelizing it across relevant machines storing partitions of the data. The paper mentions that this “query architecture is largely retained from legacy Scuba”, and relies on multiple levels of aggregators that fanout requests<label for="agg" class="margin-toggle sidenote-number"></label><input type="checkbox" id="agg" class="margin-toggle" /><span class="sidenote">The aggregator pattern also shows up in a previous paper review on <a href="https://www.micahlerner.com/2022/04/24/monarch-googles-planet-scale-in-memory-time-series-database.html">Monarch: Google’s Planet-Scale In-Memory Time Series Database</a>. </span>.</p>

<figure><img class="maincolumn-img" src="/assets/kraken/scubaarch.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<p>When executing the query, Kraken also evaluates whether it needs to access data that is no longer stored in the core system. This “out of retention” data can be stored in other Meta internal systems, but the query interface abstracts this away from the user<label for="f1query" class="margin-toggle sidenote-number"></label><input type="checkbox" id="f1query" class="margin-toggle" /><span class="sidenote">This abstraction is similar to those of <a href="http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf">F1 Query</a>, a system from Google which facilitates querying hetoregenous datasets using the same interface. </span>.</p>

<h2 id="how-is-the-research-evaluated">How is the research evaluated?</h2>

<p>The paper evaluates the new architecture in two main ways. First, the authors describe Kraken’s productionization, representing whether investments in reliability and ease of use paid off. Second, the research evaluates the performance when launched to users.</p>

<h4 id="productionaization">Productionaization</h4>

<p>The paper talk about migrating from the original system to its new incarnation with minimal impact on users. While independently deploying Kraken code (with no interaction between new and old systems) posed little problem, moving the underlying data and testing performance of the new system was more challenging.</p>

<p>One main performance hurdle that the team faced was switching to a globally-distributed deployment performing cross-region operations. Previously, each region updated its dataset separately (potentially leading to user-facing consistency differences).</p>

<p>An example of this shift was on the ingestion path - previously, Scuba sent new log entries to each region independently and didn’t wait for notification of a successful write. In contrast, Kraken writes of <em>RowBlocks</em> to <em>LogDevices</em> could span multiple regions, and <em>would</em> block on acknowledgement (introducing potential latency). The team addressed this problem by batching writes, amoritizing the latency overhead of cross-region operations.</p>

<p>Ensuring that the underlying data was completely transferred to Kraken was another challenge in the productionization process. To make sure new data was present in both Scuba and Kraken, a single group of tailers began writing data to both systems.</p>

<p>For historical data, the migration was a bit more complicated. Wholesale copying the existing dataset for a Scuba deployment could lead to duplicates if the cutover to Kraken didn’t happen instantaneously - the new tailers would start by writing logs to both Kraken and Scuba, so an entry already in Kraken could be in the Scuba copy of the dataset. Furthermore, turning off writes to Scuba without having Kraken online could lead to data loss. To solve this challenge, the authors labeled data ingested into both Kraken and Scuba. Copying data from a Scuba deployment to Kraken excluded this labeled data (as it was guaranteed to exist in the new system).</p>

<p>Lastly, reliability of Kraken at scale was one concern of switching. To test this, the paper discusses conducting “drain tests” to trigger the system’s rebalancing process (managed by <em>Shard Manager</em>), while monitoring performance and ability to adapt. Additionally, the authors forced failure in different components of the system and watch its recovery<label for="chaos" class="margin-toggle sidenote-number"></label><input type="checkbox" id="chaos" class="margin-toggle" /><span class="sidenote">This is commonly called <em>Chaos Engineering</em>, was <a href="https://arxiv.org/pdf/1702.05843.pdf">popularized by Netflix</a>. </span>.</p>

<h4 id="experiments">Experiments</h4>

<p>To understand the performance of the new system, the paper considers three main metrics: <em>query latency</em>, <em>ingestion performance</em>, and <em>recovery under failure conditions</em>.</p>

<p>When measuring <em>query latency</em>, the paper considers production datasets of different sizes and characteristics, observing that overall latency decreased between Kraken and Scuba deployments. The authors argue the Kraken’s ability to determine which shards of a dataset are under load (and scale them) is responsible for this effect - unlike Scuba’s design (which doesn’t create a deterministic mapping between data and the machines it lives on), Kraken definitively knows where data is. As a result, it can wait for only those partitions of the dataset to respond when issuing parallelized queries across nodes. Kraken’s reliance on fewer partitions also adds another advantage: aggregating and filtering results from in-scope shards at the edge incurs lower network overhead (as that data doesn’t need to be transferred over the network).</p>

<figure><img class="maincolumn-img" src="/assets/kraken/table1.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<figure><img class="maincolumn-img" src="/assets/kraken/figure5.png" /><figcaption class="maincolumn-figure"></figcaption></figure>
<figure><img class="maincolumn-img" src="/assets/kraken/figure6.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<p>Relative to Scuba, Kraken’s ingestion latency also significantly decreased<label for="factor" class="margin-toggle sidenote-number"></label><input type="checkbox" id="factor" class="margin-toggle" /><span class="sidenote">From my reading, the paper doesn’t attribute any specific factor to lowering ingestion latency. </span>, leading to fresher logs, and a better experience for users (who are relying on up to date information when querying).</p>

<figure><img class="maincolumn-img" src="/assets/kraken/table2.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<p>Lastly, the paper measures recovery from failure (in particular targeting leaf nodes), using shard availability as a proxy for recovery (more shards corresponds to a greater share of the logs being available for queries). After eliminating 10% of the shards in a deployment, Kraken is able to recover to the baseline within 3 hours - the paper doesn’t note whether they would be able to decrease this recovery time further (potentially  by taking action scaling capacity).</p>

<figure><img class="maincolumn-img" src="/assets/kraken/figure7.png" /><figcaption class="maincolumn-figure"></figcaption></figure>

<h2 id="conclusion">Conclusion</h2>

<p>The paper on Kraken contains a useful case study on evolving an existing tool to new demands. The underlying system builds on many Meta-internal pieces of infrastructure (like Shard Manager and LogDevice), and having background context from previous paper reviews, I enjoyed learning how the system was built on top of these primitives -  oftentimes papers elide the underlying implementation of the systems they are relying upon, but that is certainly not true in this case.</p>

<p>While Kraken is remarkably similar to existing databases, like Druid<label for="druid" class="margin-toggle sidenote-number"></label><input type="checkbox" id="druid" class="margin-toggle" /><span class="sidenote">Druid was the subject of a <a href="https://www.micahlerner.com/2022/05/15/druid-a-real-time-analytical-data-store.html">previous paper review</a>. </span>, but the research is novel in describing the reasoning behind different tradeoffs (with the added benefit of learning from a previous system deployed internally at scale).</p>


    <footer>
      <form
        action="https://gmail.us20.list-manage.com/subscribe/post?u=d1654f70a6addb0e9ce8afd83&amp;id=bab65ed2b1"
        method="post"
        id="mc-embedded-subscribe-form"
        name="mc-embedded-subscribe-form"
        class="validate"
        target="_blank"
        novalidate
      >
        <div id="mc_embed_signup_scroll">
          <h4>
            Follow me on
            <a href="https://twitter.com/micahlerner">Twitter</a> or subscribe
            below to get future paper reviews. Published weekly.
          </h4>
          <div>
            <input
              type="email"
              value=""
              name="EMAIL"
              class="email"
              id="tlemail"
              placeholder="email address"
              required
            />
            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
            <div style="position: absolute; left: -5000px" aria-hidden="true">
              <input
                type="text"
                name="b_d1654f70a6addb0e9ce8afd83_bab65ed2b1"
                tabindex="-1"
                value=""
              />
            </div>
            <input
              type="submit"
              value="Subscribe"
              name="subscribe"
              id="mc-embedded-subscribe"
              class="button"
            />
          </div>
        </div>
      </form>
    </footer>
  </section>
  <section>
    Found something wrong?
    <a
      href="https://github.com/mlerner/mlerner.github.io/edit/master/_posts/2023-02-27-metas-next-generation-realtime-monitoring-and-analytics-platform.md"
      >Submit a pull request!</a
    >
  </section>
</article>

<div id="portal-root">
  <div id="subscribe-container">
    <div id="gh-portal-triggerbtn-wrapper" class="gh-portal-triggerbtn-wrapper">
      <div class="gh-portal-triggerbtn-container with-label">
        <svg
          width="24"
          height="18"
          viewBox="0 0 24 18"
          fill="none"
          xmlns="http://www.w3.org/2000/svg"
          style="width: 24px; height: 24px; color: rgb(255, 255, 255)"
        >
          <path
            d="M21.75 1.5H2.25c-.828 0-1.5.672-1.5 1.5v12c0 .828.672 1.5 1.5 1.5h19.5c.828 0 1.5-.672 1.5-1.5V3c0-.828-.672-1.5-1.5-1.5zM15.687 6.975L19.5 10.5M8.313 6.975L4.5 10.5"
            stroke="#fff"
            stroke-width="1.5"
            stroke-linecap="round"
            stroke-linejoin="round"
          ></path>
          <path
            d="M22.88 2.014l-9.513 6.56C12.965 8.851 12.488 9 12 9s-.965-.149-1.367-.426L1.12 2.014"
            stroke="#fff"
            stroke-width="1.5"
            stroke-linecap="round"
            stroke-linejoin="round"
          ></path></svg
        ><span class="gh-portal-triggerbtn-label"> Subscribe </span>
      </div>
    </div>
  </div>
</div>

<div id="popup-root" style="visibility: hidden">
  <div class="inner-popup-container">
    <div class="gh-portal-popup-background"></div>
    <div class="gh-portal-popup-wrapper signup">
      <div
        class="gh-portal-popup-container gh-portal-container-narrow signup"
        tabindex="-1"
      >
        <div class="gh-portal-content signup noplan">
          <div id="closeicon-email" class="gh-portal-closeicon-container">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              viewBox="0 0 24 24"
              class="gh-portal-closeicon"
              alt="Close"
            >
              <defs>
                <style>
                  .a {
                    fill: none;
                    stroke: currentColor;
                    stroke-linecap: round;
                    stroke-linejoin: round;
                    stroke-width: 1.2px;
                  }
                </style>
              </defs>
              <path
                class="a"
                d="M.75 23.249l22.5-22.5M23.25 23.249L.75.749"
              ></path>
            </svg>
          </div>
          <header>
            <h2 class="gh-portal-main-title">Get essays a bit faster</h2>
          </header>
          <section>
            <div class="gh-portal-section">
              <form
                action="https://gmail.us20.list-manage.com/subscribe/post?u=d1654f70a6addb0e9ce8afd83&amp;id=bab65ed2b1"
                method="post"
                id="mc-embedded-subscribe-form"
                name="mc-embedded-subscribe-form"
                class="validate"
                target="_blank"
                novalidate=""
                _lpchecked="1"
              >
                <label for="mce-EMAIL"> </label>
                <p id="mce-email-describe" class="mt0">
                  I write about computer science research from the fields of
                  distributed systems and operating systems around once a week.
                </p>
                <p></p>
                <div id="mc_embed_signup_scroll">
                  <div>
                    <input
                      type="email"
                      value=""
                      name="EMAIL"
                      class="email"
                      id="tlemail"
                      placeholder="email address"
                      required=""
                    />

                    <div
                      style="position: absolute; left: -5000px"
                      aria-hidden="true"
                    >
                      <input
                        type="text"
                        name="b_d1654f70a6addb0e9ce8afd83_bab65ed2b1"
                        tabindex="-1"
                        value=""
                      />
                    </div>
                    <input
                      type="submit"
                      value="Subscribe"
                      name="subscribe"
                      id="mc-embedded-subscribe"
                      class="button"
                    />
                  </div>
                </div>
              </form>
            </div>
          </section>
        </div>
      </div>
    </div>
  </div>
</div>

<script id="mcjs">
  var subscribeButton = document.getElementById("gh-portal-triggerbtn-wrapper");
  var closeButton = document.getElementById("closeicon-email");
  var popupRoot = document.getElementById("popup-root");

  subscribeButton.addEventListener("click", function () {
    window.open('https://newsletter.micahlerner.com', '_blank');
    <!-- popupRoot.classList.toggle("m-fadeIn"); -->
    <!-- popupRoot.style.visibility = "visible"; -->
  });

  closeButton.addEventListener("click", function () {
    <!-- popupRoot.classList.toggle("m-fadeOut"); -->
    <!-- popupRoot.classList.toggle("m-fadeIn"); -->
    <!-- popupRoot.style.visibility = "hidden"; -->
  });
</script>

  </body>
</html>
